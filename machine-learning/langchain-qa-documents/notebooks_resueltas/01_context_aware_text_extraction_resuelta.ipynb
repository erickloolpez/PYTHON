{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥n de texto con base en el contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import re\n",
    "from functools import partial\n",
    "from typing import Generator\n",
    "\n",
    "from bs4 import BeautifulSoup, Doctype, NavigableString, SoupStrainer, Tag\n",
    "from dotenv import load_dotenv\n",
    "from html2text import HTML2Text\n",
    "from IPython.core.display import Markdown\n",
    "from langchain.document_loaders import DocugamiLoader, RecursiveUrlLoader\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset y funci√≥n de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_url = \"https://python.langchain.com/docs/get_started/quickstart\"\n",
    "\n",
    "load_documents = partial(\n",
    "    RecursiveUrlLoader,\n",
    "    url=doc_url,\n",
    "    max_depth=3,\n",
    "    prevent_outside=True,\n",
    "    check_response_status=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto sin tener en cuenta el contexto\n",
    "\n",
    "La primera aproximaci√≥n para extraer texto de una p√°gina web es simplemente obtener el texto de todos los elementos de la p√°gina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quickstart | ü¶úÔ∏èüîó Langchain\n",
      "Skip to main content\n",
      "ü¶úÔ∏èüîó LangChain\n",
      "Docs\n",
      "Use cases\n",
      "Integrations\n",
      "API\n",
      "Community\n",
      "Chat our docs\n",
      "LangSmith\n",
      "JS/TS Docs\n",
      "Search\n",
      "CTRL\n",
      "K\n",
      "Get started\n",
      "Introduction\n",
      "Installation\n",
      "Quickstart\n",
      "Modules\n",
      "Model I/‚ÄãO\n",
      "Retrieval\n",
      "Chains\n",
      "Memory\n",
      "Agents\n",
      "Callbacks\n",
      "Modules\n",
      "LangChain Expression Language\n",
      "Guides\n",
      "More\n",
      "Get started\n",
      "Quickstart\n",
      "On this page\n",
      "Quickstart\n",
      "Installation\n",
      "‚Äã\n",
      "To install LangChain run:\n",
      "Pip\n",
      "Conda\n",
      "pip\n",
      "install\n",
      "langchain\n",
      "conda\n",
      "install\n",
      "langchain -c conda-forge\n",
      "For more details, see our\n",
      "Installation guide\n",
      ".\n",
      "En\n"
     ]
    }
   ],
   "source": [
    "def webpage_text_extractor(html: str) -> str:\n",
    "    return BeautifulSoup(html, \"lxml\").get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "\n",
    "loader = load_documents(\n",
    "    extractor=webpage_text_extractor,\n",
    ")\n",
    "\n",
    "docs_without_data_context = loader.load()\n",
    "print(docs_without_data_context[0].page_content[:520])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto teniendo un poco de contexto\n",
    "\n",
    "El texto de la documentaci√≥n de `Langchain` est√° escrito en `Markdown`, teniendo una estructura que puede ser aprovechada para extraer el texto de manera m√°s precisa. Para ello, utilizaremos una librer√≠a que nos permita convertir el texto de `HTML` a `Markdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip to main content\n",
      "\n",
      "[ **ü¶úÔ∏èüîó LangChain**](/)[Docs](/docs/get_started/introduction)[Use\n",
      "cases](/docs/use_cases/question_answering/)[Integrations](/docs/integrations/providers)[API](https://api.python.langchain.com)[Community](/docs/community)\n",
      "\n",
      "[Chat our\n",
      "docs](https://chat.langchain.com)[LangSmith](https://smith.langchain.com)[JS/TS\n",
      "Docs](https://js.langchain.com/docs)[](https://github.com/langchain-\n",
      "ai/langchain)\n",
      "\n",
      "Search\n",
      "\n",
      "CTRLK\n",
      "\n",
      "  * [Get started](/docs/get_started)\n",
      "\n",
      "    * [Introduction](/docs/get_started/introduction)\n",
      "    * [Installation](/docs/get_started/installation)\n",
      "    * [Quickstart](/docs/get_started/quickstart)\n",
      "  * [Modules](/docs/modules/)\n",
      "\n",
      "    * [Model I/‚ÄãO](/docs/modules/model_io/)\n",
      "\n",
      "    * [Retrieval](/docs/modules/data_connection/)\n",
      "\n",
      "    * [Chains](/docs/modules/chains/)\n",
      "\n",
      "    * [Memory](/docs/modules/memory/)\n",
      "\n",
      "    * [Agents](/docs/modules/agents/)\n",
      "\n",
      "    * [Callbacks](/docs/modules/callbacks/)\n",
      "\n",
      "    * [Modules](/docs/modules/)\n",
      "  * [LangChain Expression Language](/docs/expression_language/)\n",
      "\n",
      "  * [Guides](/docs/guides)\n",
      "\n",
      "  * [More](/docs/additional_resources)\n",
      "\n",
      "  * [](/)\n",
      "  * [Get started](/docs/get_started)\n",
      "  * Quickstart\n",
      "\n",
      "On this page\n",
      "\n",
      "# Quickstart\n",
      "\n",
      "## Installation‚Äã\n",
      "\n",
      "To install LangChain run:\n",
      "\n",
      "  * Pip\n",
      "  * Conda\n",
      "\n",
      "    \n",
      "    \n",
      "    pip install langchain  \n",
      "    \n",
      "    \n",
      "    \n",
      "    conda install langchain -c conda-forge  \n",
      "    \n",
      "\n",
      "For more details, see our [Installation\n",
      "guide](/docs/get_started/installation.html).\n",
      "\n",
      "## Environment setup‚Äã\n",
      "\n",
      "Using LangChain will usually require integrations with one or more model\n",
      "providers, data stores, APIs, etc. For this example, we'll use OpenAI's model\n",
      "APIs.\n",
      "\n",
      "First we'll need to install their Python package:\n",
      "\n",
      "    \n",
      "    \n",
      "    pip install openai  \n",
      "    \n",
      "\n",
      "Accessing the API requires an API key, which you can get by creating an\n",
      "account and heading [here](https://platform.openai.com/account/api-keys). Once\n",
      "we have a key we'll want to set it as an environment variable by running:\n",
      "\n",
      "    \n",
      "    \n",
      "    export OPENAI_API_KEY=\"...\"  \n",
      "    \n",
      "\n",
      "If you'd prefer not to set an environment variable you can pass the key in\n",
      "directly via the `openai_api_key` named parameter when initiating the OpenAI\n",
      "LLM class:\n",
      "\n",
      "    \n",
      "    \n",
      "    from langchain.llms import OpenAI  \n",
      "      \n",
      "    llm = OpenAI(openai_api_key=\"...\")  \n",
      "    \n",
      "\n",
      "## Building an application‚Äã\n",
      "\n",
      "Now we can start building our language model application. LangChain provides\n",
      "many modules that can be used to build language model applications. Modules\n",
      "can be used as stand-alones in simple applications and they can be combined\n",
      "for more complex use cases.\n",
      "\n",
      "The most common and most important chain that LangChain helps create contains\n",
      "three things:\n",
      "\n",
      "  * LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
      "  * Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategi\n"
     ]
    }
   ],
   "source": [
    "def markdown_extractor(html: str) -> str:\n",
    "    html2text = HTML2Text()\n",
    "    html2text.ignore_links = False\n",
    "    html2text.ignore_images = False\n",
    "    return html2text.handle(html)\n",
    "\n",
    "\n",
    "loader = load_documents(\n",
    "    extractor=markdown_extractor,\n",
    ")\n",
    "\n",
    "docs_with_a_bit_of_context = loader.load()\n",
    "print(docs_with_a_bit_of_context[0].page_content[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto teniendo en cuenta el contexto\n",
    "\n",
    "Si bien, cuando utilizamos una librer√≠a para convertir el texto de `HTML` a `Markdown` pudimos extraer el texto de manera m√°s precisa, a√∫n hay algunos casos en los que no se logra extraer el texto de manera correcta.\n",
    "\n",
    "Es aqu√≠ donde entra en juego el dominio del problema. Con base en el conocimiento que tenemos del problema, podemos crear una funci√≥n que nos permita extraer el texto de manera m√°s precisa.\n",
    "\n",
    "Imagina que `langchain_docs_extractor` es como un obrero especializado en una f√°brica cuyo trabajo es transformar materias primas (documentos HTML) en un producto terminado (un string limpio y formateado). Este obrero usa una herramienta especial, `get_text`, como una m√°quina para procesar las materias primas en piezas utilizables, examinando cada componente de la materia prima **pieza por pieza**, y usa el mismo proceso repetidamente (**recursividad**) para descomponer los componentes en su forma m√°s simple. Al final, ensambla todas las piezas procesadas en un producto completo y hace algunos refinamientos finales antes de que el producto salga de la f√°brica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Quickstart\n",
      "\n",
      "## Installation‚Äã\n",
      "\n",
      "To install LangChain run:\n",
      "\n",
      "Pip\n",
      "```bash\n",
      "pip install langchain\n",
      "```\n",
      "\n",
      "Conda\n",
      "```bash\n",
      "conda install langchain -c conda-forge\n",
      "```\n",
      "\n",
      "For more details, see our [Installation guide](/docs/get_started/installation.html).\n",
      "\n",
      "## Environment setup‚Äã\n",
      "\n",
      "Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n",
      "\n",
      "First we'll need to install their Python package:\n",
      "\n",
      "```bash\n",
      "pip install openai\n",
      "```\n",
      "\n",
      "Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:\n",
      "\n",
      "```bash\n",
      "export OPENAI_API_KEY=\"...\"\n",
      "```\n",
      "\n",
      "If you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "llm = OpenAI(openai_api_key=\"...\")\n",
      "```\n",
      "\n",
      "## Building an application‚Äã\n",
      "\n",
      "Now we can start building our language model application. LangChain provides many modules that can be used to build language model applications.\n",
      "Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\n",
      "\n",
      "The most common and most important chain that LangChain helps create contains three things:\n",
      "\n",
      "- LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
      "- Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
      "- Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream.\n",
      "\n",
      "In this getting started guide we will cover those three components by themselves, and then go over how to combine all of them.\n",
      "Understanding these concepts will set you up well for being able to use and customize LangChain applications.\n",
      "Most LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler.\n",
      "\n",
      "## LLMs‚Äã\n",
      "\n",
      "There are two types of language models, which in LangChain are called:\n",
      "\n",
      "- LLMs: this is a language model which takes a string as input and returns a string\n",
      "- ChatModels: this is a language model which takes a list of messages as input and returns a message\n",
      "\n",
      "The input/output for LLMs is simple and easy to understand - a string.\n",
      "But what about ChatModels? The input there is a list of `ChatMessage`s, and the output is a single `ChatMessage`.\n",
      "A `ChatMessage` has two required components:\n",
      "\n",
      "- `content`: This is the content of the message.\n",
      "- `role`: This is the role of the entity from which the `ChatMessage` is coming from.\n",
      "\n",
      "LangChain provides several objects to easily disting\n"
     ]
    }
   ],
   "source": [
    "def langchain_docs_extractor(\n",
    "    html: str,\n",
    "    include_output_cells: bool,\n",
    "    path_url: str | None = None,\n",
    ") -> str:\n",
    "    soup = BeautifulSoup(\n",
    "        html,\n",
    "        \"lxml\",\n",
    "        parse_only=SoupStrainer(name=\"article\"),\n",
    "    )\n",
    "\n",
    "    # Remove all the tags that are not meaningful for the extraction.\n",
    "    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n",
    "    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n",
    "\n",
    "    # get_text() method returns the text of the tag and all its children.\n",
    "    def get_text(tag: Tag) -> Generator[str, None, None]:\n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Doctype):\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, NavigableString):\n",
    "                yield child.get_text()\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "                    text = child.get_text(strip=False)\n",
    "\n",
    "                    if text == \"API Reference:\":\n",
    "                        yield f\"> **{text}**\\n\"\n",
    "                        ul = child.find_next_sibling(\"ul\")\n",
    "                        if ul is not None and isinstance(ul, Tag):\n",
    "                            ul.attrs[\"api_reference\"] = \"true\"\n",
    "                    else:\n",
    "                        yield f\"{'#' * int(child.name[1:])} \"\n",
    "                        yield from child.get_text(strip=False)\n",
    "\n",
    "                        if path_url is not None:\n",
    "                            link = child.find(\"a\")\n",
    "                            if link is not None:\n",
    "                                yield f\" [](/{path_url}/{link.get('href')})\"\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"a\":\n",
    "                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n",
    "                elif child.name == \"img\":\n",
    "                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n",
    "                elif child.name in [\"strong\", \"b\"]:\n",
    "                    yield f\"**{child.get_text(strip=False)}**\"\n",
    "                elif child.name in [\"em\", \"i\"]:\n",
    "                    yield f\"_{child.get_text(strip=False)}_\"\n",
    "                elif child.name == \"br\":\n",
    "                    yield \"\\n\"\n",
    "                elif child.name == \"code\":\n",
    "                    parent = child.find_parent()\n",
    "                    if parent is not None and parent.name == \"pre\":\n",
    "                        classes = parent.attrs.get(\"class\", \"\")\n",
    "\n",
    "                        language = next(\n",
    "                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n",
    "                            None,\n",
    "                        )\n",
    "                        if language is None:\n",
    "                            language = \"\"\n",
    "                        else:\n",
    "                            language = language.split(\"-\")[1]\n",
    "\n",
    "                        if language in [\"pycon\", \"text\"] and not include_output_cells:\n",
    "                            continue\n",
    "\n",
    "                        lines: list[str] = []\n",
    "                        for span in child.find_all(\"span\", class_=\"token-line\"):\n",
    "                            line_content = \"\".join(\n",
    "                                token.get_text() for token in span.find_all(\"span\")\n",
    "                            )\n",
    "                            lines.append(line_content)\n",
    "\n",
    "                        code_content = \"\\n\".join(lines)\n",
    "                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n",
    "                    else:\n",
    "                        yield f\"`{child.get_text(strip=False)}`\"\n",
    "\n",
    "                elif child.name == \"p\":\n",
    "                    yield from get_text(child)\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ul\":\n",
    "                    if \"api_reference\" in child.attrs:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"> - \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    else:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"- \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ol\":\n",
    "                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n",
    "                        yield f\"{i + 1}. \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n",
    "                    \"class\", [\"\"]\n",
    "                ):\n",
    "                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n",
    "                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n",
    "                    for tab, tab_panel in zip(tabs, tab_panels):\n",
    "                        tab_name = tab.get_text(strip=True)\n",
    "                        yield f\"{tab_name}\\n\"\n",
    "                        yield from get_text(tab_panel)\n",
    "                elif child.name == \"table\":\n",
    "                    thead = child.find(\"thead\")\n",
    "                    header_exists = isinstance(thead, Tag)\n",
    "                    if header_exists:\n",
    "                        headers = thead.find_all(\"th\")\n",
    "                        if headers:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(header.get_text() for header in headers)\n",
    "                            yield \" |\\n\"\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\"----\" for _ in headers)\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    tbody = child.find(\"tbody\")\n",
    "                    tbody_exists = isinstance(tbody, Tag)\n",
    "                    if tbody_exists:\n",
    "                        for row in tbody.find_all(\"tr\"):\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name in [\"button\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    yield from get_text(child)\n",
    "\n",
    "    joined = \"\".join(get_text(soup))\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()\n",
    "\n",
    "\n",
    "loader = load_documents(\n",
    "    extractor=partial(\n",
    "        langchain_docs_extractor,\n",
    "        include_output_cells=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs_with_data_context = loader.load()\n",
    "print(docs_with_data_context[0].page_content[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo de salida es ahora en formato Markdown, lo que permite visualizarlo en cualquier editor de texto o en GitHub, ofreciendo una estructura de la informaci√≥n m√°s clara y accesible. Esta organizaci√≥n permite realizar cortes de texto con mayor precisi√≥n, facilitando as√≠ la obtenci√≥n de informaci√≥n m√°s pertinente y relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Quickstart\n",
       "\n",
       "## Installation‚Äã\n",
       "\n",
       "To install LangChain run:\n",
       "\n",
       "Pip\n",
       "```bash\n",
       "pip install langchain\n",
       "```\n",
       "\n",
       "Conda\n",
       "```bash\n",
       "conda install langchain -c conda-forge\n",
       "```\n",
       "\n",
       "For more details, see our [Installation guide](/docs/get_started/installation.html).\n",
       "\n",
       "## Environment setup‚Äã\n",
       "\n",
       "Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n",
       "\n",
       "First we'll need to install their Python package:\n",
       "\n",
       "```bash\n",
       "pip install openai\n",
       "```\n",
       "\n",
       "Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:\n",
       "\n",
       "```bash\n",
       "export OPENAI_API_KEY=\"...\"\n",
       "```\n",
       "\n",
       "If you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:\n",
       "\n",
       "```python\n",
       "from langchain.llms import OpenAI\n",
       "\n",
       "llm = OpenAI(openai_api_key=\"...\")\n",
       "```\n",
       "\n",
       "## Building an application‚Äã\n",
       "\n",
       "Now we can start building our language model application. LangChain provides many modules that can be used to build language model applications.\n",
       "Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\n",
       "\n",
       "The most common and most important chain that LangChain helps create contains three things:\n",
       "\n",
       "- LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
       "- Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
       "- Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream.\n",
       "\n",
       "In this getting started guide we will cover those three components by themselves, and then go over how to combine all of them.\n",
       "Understanding these concepts will set you up well for being able to use and customize LangChain applications.\n",
       "Most LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler.\n",
       "\n",
       "## LLMs‚Äã\n",
       "\n",
       "There are two types of language models, which in LangChain are called:\n",
       "\n",
       "- LLMs: this is a language model which takes a string as input and returns a string\n",
       "- ChatModels: this is a language model which takes a list of messages as input and returns a message\n",
       "\n",
       "The input/output for LLMs is simple and easy to understand - a string.\n",
       "But what about ChatModels? The input there is a list of `ChatMessage`s, and the output is a single `ChatMessage`.\n",
       "A `ChatMessage` has two required components:\n",
       "\n",
       "- `content`: This is the content of the message.\n",
       "- `role`: This is the role of the entity from which the `ChatMessage` is coming from.\n",
       "\n",
       "LangChain provides several objects to easily distinguish between different roles:\n",
       "\n",
       "- `HumanMessage`: A `ChatMessage` coming from a human/user.\n",
       "- `AIMessage`: A `ChatMessage` coming from an AI/assistant.\n",
       "- `SystemMessage`: A `ChatMessage` coming from the system.\n",
       "- `FunctionMessage`: A `ChatMessage` coming from a function call.\n",
       "\n",
       "If none of those roles sound right, there is also a `ChatMessage` class where you can specify the role manually.\n",
       "For more information on how to use these different messages most effectively, see our prompting guide.\n",
       "\n",
       "LangChain provides a standard interface for both, but it's useful to understand this difference in order to construct prompts for a given language model.\n",
       "The standard interface that LangChain provides has two methods:\n",
       "\n",
       "- `predict`: Takes in a string, returns a string\n",
       "- `predict_messages`: Takes in a list of messages, returns a message.\n",
       "\n",
       "Let's see how to work with these different types of models and these different types of inputs.\n",
       "First, let's import an LLM and a ChatModel.\n",
       "\n",
       "```python\n",
       "from langchain.llms import OpenAI\n",
       "from langchain.chat_models import ChatOpenAI\n",
       "\n",
       "llm = OpenAI()\n",
       "chat_model = ChatOpenAI()\n",
       "\n",
       "llm.predict(\"hi!\")\n",
       ">>> \"Hi\"\n",
       "\n",
       "chat_model.predict(\"hi!\")\n",
       ">>> \"Hi\"\n",
       "```\n",
       "\n",
       "The `OpenAI` and `ChatOpenAI` objects are basically just configuration objects.\n",
       "You can initialize them with parameters like `temperature` and others, and pass them around.\n",
       "\n",
       "Next, let's use the `predict` method to run over a string input.\n",
       "\n",
       "```python\n",
       "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
       "\n",
       "llm.predict(text)\n",
       "# >> Feetful of Fun\n",
       "\n",
       "chat_model.predict(text)\n",
       "# >> Socks O'Color\n",
       "```\n",
       "\n",
       "Finally, let's use the `predict_messages` method to run over a list of messages.\n",
       "\n",
       "```python\n",
       "from langchain.schema import HumanMessage\n",
       "\n",
       "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
       "messages = [HumanMessage(content=text)]\n",
       "\n",
       "llm.predict_messages(messages)\n",
       "# >> Feetful of Fun\n",
       "\n",
       "chat_model.predict_messages(messages)\n",
       "# >> Socks O'Color\n",
       "```\n",
       "\n",
       "For both these methods, you can also pass in parameters as key word arguments.\n",
       "For example, you could pass in `temperature=0` to adjust the temperature that is used from what the object was configured with.\n",
       "Whatever values are passed in during run time will always override what the object was configured with.\n",
       "\n",
       "## Prompt templates‚Äã\n",
       "\n",
       "Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n",
       "\n",
       "In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\n",
       "\n",
       "PromptTemplates help with exactly this!\n",
       "They bundle up all the logic for going from user input into a fully formatted prompt.\n",
       "This can start off very simple - for example, a prompt to produce the above string would just be:\n",
       "\n",
       "```python\n",
       "from langchain.prompts import PromptTemplate\n",
       "\n",
       "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
       "prompt.format(product=\"colorful socks\")\n",
       "```\n",
       "\n",
       "```pycon\n",
       "What is a good name for a company that makes colorful socks?\n",
       "```\n",
       "\n",
       "However, the advantages of using these over raw string formatting are several.\n",
       "You can \"partial\" out variables - e.g. you can format only some of the variables at a time.\n",
       "You can compose them together, easily combining different templates into a single prompt.\n",
       "For explanations of these functionalities, see the [section on prompts](/docs/modules/model_io/prompts) for more detail.\n",
       "\n",
       "PromptTemplates can also be used to produce a list of messages.\n",
       "In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc)\n",
       "Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates.\n",
       "Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content.\n",
       "Let's take a look at this below:\n",
       "\n",
       "```python\n",
       "from langchain.prompts.chat import ChatPromptTemplate\n",
       "\n",
       "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
       "human_template = \"{text}\"\n",
       "\n",
       "chat_prompt = ChatPromptTemplate.from_messages([\n",
       "    (\"system\", template),\n",
       "    (\"human\", human_template),\n",
       "])\n",
       "\n",
       "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
       "```\n",
       "\n",
       "```pycon\n",
       "[\n",
       "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\", additional_kwargs={}),\n",
       "    HumanMessage(content=\"I love programming.\")\n",
       "]\n",
       "```\n",
       "\n",
       "ChatPromptTemplates can also be constructed in other ways - see the [section on prompts](/docs/modules/model_io/prompts) for more detail.\n",
       "\n",
       "## Output parsers‚Äã\n",
       "\n",
       "OutputParsers convert the raw output of an LLM into a format that can be used downstream.\n",
       "There are few main type of OutputParsers, including:\n",
       "\n",
       "- Convert text from LLM -> structured information (e.g. JSON)\n",
       "- Convert a ChatMessage into just a string\n",
       "- Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n",
       "\n",
       "For full information on this, see the [section on output parsers](/docs/modules/model_io/output_parsers)\n",
       "\n",
       "In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list.\n",
       "\n",
       "```python\n",
       "from langchain.schema import BaseOutputParser\n",
       "\n",
       "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
       "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
       "\n",
       "    def parse(self, text: str):\n",
       "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
       "        return text.strip().split(\", \")\n",
       "\n",
       "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
       "# >> ['hi', 'bye']\n",
       "```\n",
       "\n",
       "## PromptTemplate + LLM + OutputParser‚Äã\n",
       "\n",
       "We can now combine all these into one chain.\n",
       "This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser.\n",
       "This is a convenient way to bundle up a modular piece of logic.\n",
       "Let's see it in action!\n",
       "\n",
       "```python\n",
       "from langchain.chat_models import ChatOpenAI\n",
       "from langchain.prompts.chat import ChatPromptTemplate\n",
       "from langchain.schema import BaseOutputParser\n",
       "\n",
       "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
       "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
       "\n",
       "    def parse(self, text: str):\n",
       "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
       "        return text.strip().split(\", \")\n",
       "\n",
       "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
       "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
       "ONLY return a comma separated list, and nothing more.\"\"\"\n",
       "human_template = \"{text}\"\n",
       "\n",
       "chat_prompt = ChatPromptTemplate.from_messages([\n",
       "    (\"system\", template),\n",
       "    (\"human\", human_template),\n",
       "])\n",
       "chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
       "chain.invoke({\"text\": \"colors\"})\n",
       "# >> ['red', 'blue', 'green', 'yellow', 'orange']\n",
       "```\n",
       "\n",
       "Note that we are using the `|` syntax to join these components together.\n",
       "This `|` syntax is called the LangChain Expression Language.\n",
       "To learn more about this syntax, read the documentation [here](/docs/expression_language).\n",
       "\n",
       "## Next steps‚Äã\n",
       "\n",
       "This is it!\n",
       "We've now gone over how to create the core building block of LangChain applications.\n",
       "There is a lot more nuance in all these components (LLMs, prompts, output parsers) and a lot more different components to learn about as well.\n",
       "To continue on your journey:\n",
       "\n",
       "- [Dive deeper](/docs/modules/model_io) into LLMs, prompts, and output parsers\n",
       "- Learn the other [key components](/docs/modules)\n",
       "- Read up on [LangChain Expression Language](/docs/expression_language) to learn how to chain these components together\n",
       "- Check out our [helpful guides](/docs/guides) for detailed walkthroughs on particular topics\n",
       "- Explore [end-to-end use cases](/docs/use_cases)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(docs_with_data_context[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF / DOCX / DOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, vamos a emplear algunos archivos de muestra proporcionados por [Docugami](https://www.docugami.com/). Dichos archivos representan el producto de la extracci√≥n de texto de documentos aut√©nticos, en particular, de archivos PDF relativos a contratos de arrendamiento comercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/docugami/commercial_lease/TruTone Lane 6.xml'),\n",
       " PosixPath('../data/docugami/commercial_lease/TruTone Lane 5.xml'),\n",
       " PosixPath('../data/docugami/commercial_lease/TruTone Lane 4.xml'),\n",
       " PosixPath('../data/docugami/commercial_lease/TruTone Lane 1.xml'),\n",
       " PosixPath('../data/docugami/commercial_lease/TruTone Lane 3.xml'),\n",
       " PosixPath('../data/docugami/commercial_lease/TruTone Lane 2.xml')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lease_data_dir = pathlib.Path(\"../data/docugami/commercial_lease\")\n",
    "lease_files = list(lease_data_dir.glob(\"*.xml\"))\n",
    "lease_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, carguemos los documentos de muestra y veamos qu√© propiedades tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded 1108 documents.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DocugamiLoader(\n",
    "    docset_id=None,\n",
    "    access_token=None,\n",
    "    document_ids=None,\n",
    "    file_paths=lease_files,\n",
    ")\n",
    "\n",
    "lease_docs = loader.load()\n",
    "f\"Loaded {len(lease_docs)} documents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La metadata obtenida del documento incluye los siguientes elementos:\n",
    "\n",
    "- `id`, `source_id` y `name`: Estos campos identifican de manera un√≠voca al documento y al fragmento de texto que se ha extra√≠do de √©l.\n",
    "- `xpath`: Es el `XPath` correspondiente dentro de la representaci√≥n XML del documento. Se refiere espec√≠ficamente al fragmento extra√≠do. Este campo es √∫til para referenciar directamente las citas del fragmento real dentro del documento XML.\n",
    "- `structure`: Incluye los atributos estructurales del fragmento, tales como `p`, `h1`, `div`, `table`, `td`, entre otros. Es √∫til para filtrar ciertos tipos de fragmentos, en caso de que el usuario los requiera.\n",
    "- `tag`: Representa la etiqueta sem√°ntica para el fragmento. Se genera utilizando diversas t√©cnicas, tanto generativas como extractivas, para determinar el significado del fragmento en cuesti√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xpath': '/dg:chunk/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/docset:Lease',\n",
       " 'id': 'TruTone Lane 6.xml',\n",
       " 'name': 'TruTone Lane 6.xml',\n",
       " 'source': 'TruTone Lane 6.xml',\n",
       " 'structure': 'p',\n",
       " 'tag': 'Lease'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lease_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docugami` tambi√©n posee la capacidad de asistir en la extracci√≥n de metadatos espec√≠ficos para cada `chunk` o fragmento de nuestros documentos. A continuaci√≥n, se presenta un ejemplo de c√≥mo se extraen y representan estos metadatos:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'xpath': '/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/docset:LeaseParties',\n",
    "    'id': 'v1bvgaozfkak',\n",
    "    'source': 'TruTone Lane 2.docx',\n",
    "    'structure': 'p',\n",
    "    'tag': 'LeaseParties',\n",
    "    'Lease Date': 'April 24 \\n\\n ,',\n",
    "    'Landlord': 'BUBBA CENTER PARTNERSHIP',\n",
    "    'Tenant': 'Truetone Lane LLC',\n",
    "    'Lease Parties': 'Este ACUERDO DE ARRENDAMIENTO DE OFICINA (el \"Contrato\") es celebrado por y entre BUBBA CENTER PARTNERSHIP (\"Arrendador\"), y Truetone Lane LLC, una compa√±√≠a de responsabilidad limitada de Delaware (\"Arrendatario\").'\n",
    "}\n",
    "```\n",
    "\n",
    "Los metadatos adicionales, como los mostrados arriba, pueden ser extremadamente √∫tiles cuando se implementan `self-retrievers`, los cuales ser√°n explorados adetalle m√°s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga tus documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prefieres utilizar tus propios documentos, puedes cargarlos a trav√©s de la interfaz gr√°fica de [Docugami](https://www.docugami.com/). Una vez cargados, necesitar√°s asignar cada uno a un `docset`. Un `docset` es un conjunto de documentos que presentan una estructura an√°loga. Por ejemplo, todos los contratos de arrendamiento comercial por lo general poseen estructuras similares, por lo que pueden ser agrupados en un √∫nico `docset`.\n",
    "\n",
    "Despu√©s de crear tu `docset`, los documentos cargados ser√°n procesados y estar√°n disponibles para su acceso mediante la API de `Docugami`.\n",
    "\n",
    "Para recuperar los `ids` de tus documentos y de sus correspondientes `docsets`, puedes ejecutar el siguiente comando:\n",
    "\n",
    "```bash\n",
    "curl --header \"Authorization: Bearer {YOUR_DOCUGAMI_TOKEN}\" \\\n",
    "  https://api.docugami.com/v1preview1/documents\n",
    "```\n",
    "\n",
    "Este comando te facilitar√° el acceso a la informaci√≥n relevante, optimizando as√≠ la administraci√≥n y organizaci√≥n de tus documentos dentro de `Docugami`.\n",
    "\n",
    "Una vez hayas extra√≠do los `ids` de tus documentos o de los `docsets`, podr√°s emplearlos para acceder a la informaci√≥n de tus documentos utilizando el `DocugamiLoader` de `Langchain`. Esto te permitir√° manipular y gestionar tus documentos dentro de tu aplicaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DocugamiLoader(\n",
    "    docset_id=\"xpfpiyl7cep2\",\n",
    "    document_ids=None,\n",
    "    file_paths=None,\n",
    ")\n",
    "\n",
    "papers_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LostintheMiddle\n",
      "chunk\n",
      "chunk\n",
      "Abstract\n",
      "chunk\n",
      "AnImportantAndFlexibleBuildingBlock\n",
      "TheseUse-cases\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "Figure1\n",
      "chunk\n",
      "Transformers\n",
      "Extended-contextLanguageModels\n",
      "TrolledExperiments\n",
      "ADistinctiveU-shapedPerformance\n",
      "_5-turboS\n",
      "LanguageModels\n",
      "LanguageModels\n",
      "ACaseStudy\n",
      "_2LanguageModels\n",
      "IncreasingLanguageModelMaximumContext\n",
      "OurGoal\n",
      "chunk\n",
      "ModelPerformance\n",
      "OurMulti-documentQuestion\n",
      "ThisTask\n",
      "Naturalquestions-open\n",
      "RandomDocuments\n",
      "AHigh-qualityAnswer\n",
      "Asian\n",
      "chunk\n",
      "chunk\n",
      "AHigh-qualityAnswer\n",
      "chunk\n",
      "Document\n",
      "Norwegian\n",
      "Question\n",
      "chunk\n",
      "Figure3\n",
      "TheInputContextLength\n",
      "Kandpal\n",
      "SearchResults\n",
      "AHigh-qualityAnswer\n",
      "chunk\n",
      "Asian\n",
      "Question\n",
      "chunk\n",
      "Figure4\n",
      "TheNaturalquestionsAnnotations\n",
      "chunk\n",
      "AMaximumContextLength\n",
      "chunk\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "_7-cell\n",
      "ExtracttheValueCorrespondingtotheSpecifiedKeyintheJSONObjectBelowVa\n",
      "td\n",
      "ExtracttheValueCorrespondingtotheSpecifiedKeyintheJSONObjectBelowEx\n",
      "ExtracttheValueCorrespondingtotheSpecifiedKeyintheJSONObjectBelow\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Figure9-cell\n",
      "td\n",
      "td\n",
      "td\n",
      "The-art\n",
      "AMaximumContextLength\n",
      "ClosedModels\n",
      "Gpt-35-turbo\n",
      "TheAnthropicApi\n",
      "InputContexts\n",
      "ModelPerformance\n",
      "ASubset\n",
      "Figure6\n",
      "Contexts\n",
      "ModelPerformance\n",
      "PerformanceDecrease\n",
      "Extended-contextModels\n",
      "LanguageModels\n",
      "OurSyntheticKey-valueRetrievalTask\n",
      "OurSyntheticKey-valueRetrievalTask\n",
      "Figure8\n",
      "PresentPotentialConfounders\n",
      "Key-valueRetrievalPerformance\n",
      "chunk\n",
      "TheSyntheticKey-valueRetrievalTask\n",
      "TheKey-valueRetrievalTask\n",
      "The140Key-valueSetting\n",
      "chunk\n",
      "OurMulti-documentQuestion\n",
      "chunk\n",
      "TheOpenModels\n",
      "OurExperiments\n",
      "tr\n",
      "chunk\n",
      "TheEnd\n",
      "TheSameSetting\n",
      "TheModels\n",
      "Mpt-30bMpt-30b-instruct\n",
      "chunk\n",
      "Formation\n",
      "TheseObservations\n",
      "chunk\n",
      "PracticalSettings\n",
      "Models\n",
      "Figure\n",
      "td\n",
      "chunk\n",
      "chunk\n",
      "ARichLine\n",
      "chunk\n",
      "ThePioneeringWork\n",
      "TheU-shapedCurve\n",
      "ASeries\n",
      "Instruction-tuning\n",
      "SewonMin\n",
      "AviArampatzis\n",
      "IzBeltagy\n",
      "HyungWonChung\n",
      "ZihangDai\n",
      "Micha√∏Daniluk\n",
      "TriDao\n",
      "chunk\n",
      "chunk\n",
      "DanielYFu\n",
      "AlbertGu\n",
      "chunk\n",
      "Long-textUnderstanding\n",
      "GautierIzacard\n",
      "GautierIzacard\n",
      "NikhilKandpal\n",
      "UrvashiKhandelwal\n",
      "chunk\n",
      "Field\n",
      "KentonLee\n",
      "DachengLi\n",
      "AlexMallen\n",
      "SewonMin\n",
      "BennetBMurdockJr\n",
      "JoeOConnor\n",
      "DimitrisPapailiopoulos\n",
      "chunk\n",
      "HaoPeng\n",
      "FabioPetroni\n",
      "MichaelPoli\n",
      "OfirPress\n",
      "OfirPress\n",
      "GuanghuiQin\n",
      "chunk\n",
      "IneLee\n",
      "YoavLevine\n",
      "chunk\n",
      "chunk\n",
      "ChinnadhuraiSankar\n",
      "TimoSchick\n",
      "chunk\n",
      "OmerLevy\n",
      "UriShaham\n",
      "VatsalSharan\n",
      "WeijiaShi\n",
      "chunk\n",
      "chunk\n",
      "KalpeshKrishna\n",
      "YiTay\n",
      "YiTay\n",
      "MostafaDehghani\n",
      "HugoTouvron\n",
      "AshishVaswani\n",
      "SinongWang\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "chunk\n",
      "ManzilZaheer\n",
      "chunk\n",
      "PastWork\n",
      "chunk\n",
      "chunk\n",
      "td\n",
      "td\n",
      "Figure15\n",
      "Multi-documentQuestion\n",
      "chunk\n",
      "chunk\n",
      "td\n",
      "td\n",
      "chunk\n",
      "OurPrompt\n",
      "chunk\n",
      "ASubset\n",
      "chunk\n",
      "Figure17\n",
      "_1st5th10th15th20thPosition\n",
      "td\n",
      "chunk\n",
      "Table\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Table1\n",
      "Table\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Table2\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Table3\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Table4\n",
      "chunk\n",
      "ModelPerformance\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Table\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Table\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "td\n",
      "Table\n"
     ]
    }
   ],
   "source": [
    "lost_in_the_middle_paper_docs = [\n",
    "    doc for doc in papers_docs if doc.metadata[\"source\"] == \"2307.03172.pdf\"\n",
    "]\n",
    "for doc in lost_in_the_middle_paper_docs:\n",
    "    print(doc.metadata[\"tag\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
