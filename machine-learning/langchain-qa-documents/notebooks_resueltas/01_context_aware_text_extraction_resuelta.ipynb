{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥n de texto con base en el contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import re\n",
    "from functools import partial\n",
    "from typing import Generator\n",
    "\n",
    "from bs4 import BeautifulSoup, Doctype, NavigableString, SoupStrainer, Tag\n",
    "from dotenv import load_dotenv\n",
    "from html2text import HTML2Text\n",
    "from IPython.core.display import Markdown\n",
    "from langchain.document_loaders import DocugamiLoader, RecursiveUrlLoader\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset y funci√≥n de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_url = \"https://python.langchain.com/docs/tutorials/\"\n",
    "\n",
    "load_documents = partial(\n",
    "    RecursiveUrlLoader,\n",
    "    url=doc_url,\n",
    "    max_depth=3,\n",
    "    prevent_outside=True,\n",
    "    check_response_status=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto sin tener en cuenta el contexto\n",
    "\n",
    "La primera aproximaci√≥n para extraer texto de una p√°gina web es simplemente obtener el texto de todos los elementos de la p√°gina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutorials | ü¶úÔ∏èüîó LangChain\n",
      "Skip to main content\n",
      "We are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.\n",
      "Join our team!\n",
      "Integrations\n",
      "API Reference\n",
      "More\n",
      "Contributing\n",
      "People\n",
      "Error reference\n",
      "LangSmith\n",
      "LangGraph\n",
      "LangChain Hub\n",
      "LangChain JS/TS\n",
      "v0.3\n",
      "v0.3\n",
      "v0.2\n",
      "v0.1\n",
      "üí¨\n",
      "Search\n",
      "Introduction\n",
      "Tutorials\n",
      "Build a Question Answering application over a Graph Database\n",
      "Tutorials\n",
      "Build a simple LLM application with chat models and prompt templates\n",
      "Build a Chatbot\n",
      "Build a Retrieval Augmented Generation (RAG) Ap\n"
     ]
    }
   ],
   "source": [
    "def webpage_text_extractor(html: str) -> str:\n",
    "    return BeautifulSoup(html, \"lxml\").get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "\n",
    "loader = load_documents(\n",
    "    extractor=webpage_text_extractor,\n",
    ")\n",
    "\n",
    "docs_without_data_context = loader.load()\n",
    "print(docs_without_data_context[0].page_content[:520])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto teniendo un poco de contexto\n",
    "\n",
    "El texto de la documentaci√≥n de `Langchain` est√° escrito en `Markdown`, teniendo una estructura que puede ser aprovechada para extraer el texto de manera m√°s precisa. Para ello, utilizaremos una librer√≠a que nos permita convertir el texto de `HTML` a `Markdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip to main content\n",
      "\n",
      "**We are growing and hiring for multiple roles for LangChain, LangGraph and\n",
      "LangSmith.[ Join our team!](https://www.langchain.com/careers)**\n",
      "\n",
      "[![ü¶úÔ∏èüîó LangChain](/img/brand/wordmark.png)![ü¶úÔ∏èüîó\n",
      "LangChain](/img/brand/wordmark-\n",
      "dark.png)](/)[Integrations](/docs/integrations/providers/)[API\n",
      "Reference](https://python.langchain.com/api_reference/)\n",
      "\n",
      "More\n",
      "\n",
      "  * [Contributing](/docs/contributing/)\n",
      "  * [People](/docs/people/)\n",
      "  * [Error reference](/docs/troubleshooting/errors/)\n",
      "  * * * *\n",
      "\n",
      "  * [LangSmith](https://docs.smith.langchain.com)\n",
      "  * [LangGraph](https://langchain-ai.github.io/langgraph/)\n",
      "  * [LangChain Hub](https://smith.langchain.com/hub)\n",
      "  * [LangChain JS/TS](https://js.langchain.com)\n",
      "\n",
      "v0.3\n",
      "\n",
      "  * [v0.3](/docs/introduction/)\n",
      "  * [v0.2](https://python.langchain.com/v0.2/docs/introduction)\n",
      "  * [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)\n",
      "\n",
      "[üí¨](https://chat.langchain.com)[](https://github.com/langchain-ai/langchain)\n",
      "\n",
      "Search\n",
      "\n",
      "  * [Introduction](/docs/introduction/)\n",
      "  * [Tutorials](/docs/tutorials/)\n",
      "\n",
      "    * [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)\n",
      "    * [Tutorials](/docs/tutorials/)\n",
      "    * [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)\n",
      "    * [Build a Chatbot](/docs/tutorials/chatbot/)\n",
      "    * [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)\n",
      "    * [Build an Extraction Chain](/docs/tutorials/extraction/)\n",
      "    * [Build an Agent](/docs/tutorials/agents/)\n",
      "    * [Tagging](/docs/tutorials/classification/)\n",
      "    * [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)\n",
      "    * [Build a semantic search engine](/docs/tutorials/retrievers/)\n",
      "    * [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)\n",
      "    * [Summarize Text](/docs/tutorials/summarization/)\n",
      "  * [How-to guides](/docs/how_to/)\n",
      "\n",
      "    * [How-to guides](/docs/how_to/)\n",
      "    * [How to use tools in a chain](/docs/how_to/tools_chain/)\n",
      "    * [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n",
      "    * [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n",
      "    * [How to use example selectors](/docs/how_to/example_selectors/)\n",
      "    * [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n",
      "    * [How to invoke runnables in parallel](/docs/how_to/parallel/)\n",
      "    * [How to stream chat model responses](/docs/how_to/chat_streaming/)\n",
      "    * [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n",
      "    * [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n",
      "    * [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n",
      "    * [How to do tool/function calling](/docs/how_to/function_calling/)\n",
      "    * [How to install LangChain packages](/docs/how_to/installation/)\n",
      "    * [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n",
      "    * [How to use few shot examples](/docs/how_t\n"
     ]
    }
   ],
   "source": [
    "def markdown_extractor(html: str) -> str:\n",
    "    html2text = HTML2Text()\n",
    "    html2text.ignore_links = False\n",
    "    html2text.ignore_images = False\n",
    "    return html2text.handle(html)\n",
    "\n",
    "\n",
    "loader = load_documents(\n",
    "    extractor=markdown_extractor,\n",
    ")\n",
    "\n",
    "docs_with_a_bit_of_context = loader.load()\n",
    "print(docs_with_a_bit_of_context[0].page_content[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracci√≥n de texto teniendo en cuenta el contexto\n",
    "\n",
    "Si bien, cuando utilizamos una librer√≠a para convertir el texto de `HTML` a `Markdown` pudimos extraer el texto de manera m√°s precisa, a√∫n hay algunos casos en los que no se logra extraer el texto de manera correcta.\n",
    "\n",
    "Es aqu√≠ donde entra en juego el dominio del problema. Con base en el conocimiento que tenemos del problema, podemos crear una funci√≥n que nos permita extraer el texto de manera m√°s precisa.\n",
    "\n",
    "Imagina que `langchain_docs_extractor` es como un obrero especializado en una f√°brica cuyo trabajo es transformar materias primas (documentos HTML) en un producto terminado (un string limpio y formateado). Este obrero usa una herramienta especial, `get_text`, como una m√°quina para procesar las materias primas en piezas utilizables, examinando cada componente de la materia prima **pieza por pieza**, y usa el mismo proceso repetidamente (**recursividad**) para descomponer los componentes en su forma m√°s simple. Al final, ensambla todas las piezas procesadas en un producto completo y hace algunos refinamientos finales antes de que el producto salga de la f√°brica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/index.mdx)# Tutorials\n",
      "\n",
      "New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\n",
      "\n",
      "## Get started‚Äã\n",
      "\n",
      "Familiarize yourself with LangChain's open-source components by building simple applications.\n",
      "\n",
      "If you're looking to get started with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\n",
      "or other LangChain components from a specific provider, check out our supported [integrations](/docs/integrations/providers/).\n",
      "\n",
      "- [Chat models and prompts](/docs/tutorials/llm_chain/): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates/) and [chat models](/docs/concepts/chat_models/).\n",
      "- [Semantic search](/docs/tutorials/retrievers/): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders/), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).\n",
      "- [Classification](/docs/tutorials/classification/): Classify text into categories or labels using [chat models](/docs/concepts/chat_models/) with [structured outputs](/docs/concepts/structured_outputs/).\n",
      "- [Extraction](/docs/tutorials/extraction/): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models/) and [few-shot examples](/docs/concepts/few_shot_prompting/).\n",
      "\n",
      "Refer to the [how-to guides](/docs/how_to/) for more detail on using all LangChain components.\n",
      "\n",
      "## Orchestration‚Äã\n",
      "\n",
      "Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.\n",
      "\n",
      "- [Chatbots](/docs/tutorials/chatbot/): Build a chatbot that incorporates memory.\n",
      "- [Agents](/docs/tutorials/agents/): Build an agent that interacts with external tools.\n",
      "- [Retrieval Augmented Generation (RAG) Part 1](/docs/tutorials/rag/): Build an application that uses your own documents to inform its responses.\n",
      "- [Retrieval Augmented Generation (RAG) Part 2](/docs/tutorials/qa_chat_history/): Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\n",
      "- [Question-Answering with SQL](/docs/tutorials/sql_qa/): Build a question-answering system that executes SQL queries to inform its responses.\n",
      "- [Summarization](/docs/tutorials/summarization/): Generate summaries of (potentially long) texts.\n",
      "- [Question-Answering with Graph Databases](/docs/tutorials/graph/): Build a question-answering system that queries a graph database to inform its responses.\n",
      "\n",
      "## LangSmith‚Äã\n",
      "\n",
      "LangSmith allows you to closely trace, monitor and evaluate your LLM application.\n",
      "It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n",
      "\n",
      "LangSmith documentation is hosted on a separate site.\n",
      "You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).\n",
      "\n",
      "### E\n"
     ]
    }
   ],
   "source": [
    "def langchain_docs_extractor(\n",
    "    html: str,\n",
    "    include_output_cells: bool,\n",
    "    path_url: str | None = None,\n",
    ") -> str:\n",
    "    soup = BeautifulSoup(\n",
    "        html,\n",
    "        \"lxml\",\n",
    "        parse_only=SoupStrainer(name=\"article\"),\n",
    "    )\n",
    "\n",
    "    # Remove all the tags that are not meaningful for the extraction.\n",
    "    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n",
    "    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n",
    "\n",
    "    # get_text() method returns the text of the tag and all its children.\n",
    "    def get_text(tag: Tag) -> Generator[str, None, None]:\n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Doctype):\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, NavigableString):\n",
    "                yield child.get_text()\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "                    text = child.get_text(strip=False)\n",
    "\n",
    "                    if text == \"API Reference:\":\n",
    "                        yield f\"> **{text}**\\n\"\n",
    "                        ul = child.find_next_sibling(\"ul\")\n",
    "                        if ul is not None and isinstance(ul, Tag):\n",
    "                            ul.attrs[\"api_reference\"] = \"true\"\n",
    "                    else:\n",
    "                        yield f\"{'#' * int(child.name[1:])} \"\n",
    "                        yield from child.get_text(strip=False)\n",
    "\n",
    "                        if path_url is not None:\n",
    "                            link = child.find(\"a\")\n",
    "                            if link is not None:\n",
    "                                yield f\" [](/{path_url}/{link.get('href')})\"\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"a\":\n",
    "                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n",
    "                elif child.name == \"img\":\n",
    "                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n",
    "                elif child.name in [\"strong\", \"b\"]:\n",
    "                    yield f\"**{child.get_text(strip=False)}**\"\n",
    "                elif child.name in [\"em\", \"i\"]:\n",
    "                    yield f\"_{child.get_text(strip=False)}_\"\n",
    "                elif child.name == \"br\":\n",
    "                    yield \"\\n\"\n",
    "                elif child.name == \"code\":\n",
    "                    parent = child.find_parent()\n",
    "                    if parent is not None and parent.name == \"pre\":\n",
    "                        classes = parent.attrs.get(\"class\", \"\")\n",
    "\n",
    "                        language = next(\n",
    "                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n",
    "                            None,\n",
    "                        )\n",
    "                        if language is None:\n",
    "                            language = \"\"\n",
    "                        else:\n",
    "                            language = language.split(\"-\")[1]\n",
    "\n",
    "                        if language in [\"pycon\", \"text\"] and not include_output_cells:\n",
    "                            continue\n",
    "\n",
    "                        lines: list[str] = []\n",
    "                        for span in child.find_all(\"span\", class_=\"token-line\"):\n",
    "                            line_content = \"\".join(\n",
    "                                token.get_text() for token in span.find_all(\"span\")\n",
    "                            )\n",
    "                            lines.append(line_content)\n",
    "\n",
    "                        code_content = \"\\n\".join(lines)\n",
    "                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n",
    "                    else:\n",
    "                        yield f\"`{child.get_text(strip=False)}`\"\n",
    "\n",
    "                elif child.name == \"p\":\n",
    "                    yield from get_text(child)\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ul\":\n",
    "                    if \"api_reference\" in child.attrs:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"> - \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    else:\n",
    "                        for li in child.find_all(\"li\", recursive=False):\n",
    "                            yield \"- \"\n",
    "                            yield from get_text(li)\n",
    "                            yield \"\\n\"\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ol\":\n",
    "                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n",
    "                        yield f\"{i + 1}. \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n",
    "                    \"class\", [\"\"]\n",
    "                ):\n",
    "                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n",
    "                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n",
    "                    for tab, tab_panel in zip(tabs, tab_panels):\n",
    "                        tab_name = tab.get_text(strip=True)\n",
    "                        yield f\"{tab_name}\\n\"\n",
    "                        yield from get_text(tab_panel)\n",
    "                elif child.name == \"table\":\n",
    "                    thead = child.find(\"thead\")\n",
    "                    header_exists = isinstance(thead, Tag)\n",
    "                    if header_exists:\n",
    "                        headers = thead.find_all(\"th\")\n",
    "                        if headers:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(header.get_text() for header in headers)\n",
    "                            yield \" |\\n\"\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\"----\" for _ in headers)\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    tbody = child.find(\"tbody\")\n",
    "                    tbody_exists = isinstance(tbody, Tag)\n",
    "                    if tbody_exists:\n",
    "                        for row in tbody.find_all(\"tr\"):\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name in [\"button\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    yield from get_text(child)\n",
    "\n",
    "    joined = \"\".join(get_text(soup))\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()\n",
    "\n",
    "\n",
    "loader = load_documents(\n",
    "    extractor=partial(\n",
    "        langchain_docs_extractor,\n",
    "        include_output_cells=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs_with_data_context = loader.load()\n",
    "print(docs_with_data_context[0].page_content[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo de salida es ahora en formato Markdown, lo que permite visualizarlo en cualquier editor de texto o en GitHub, ofreciendo una estructura de la informaci√≥n m√°s clara y accesible. Esta organizaci√≥n permite realizar cortes de texto con mayor precisi√≥n, facilitando as√≠ la obtenci√≥n de informaci√≥n m√°s pertinente y relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/index.mdx)# Tutorials\n",
       "\n",
       "New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\n",
       "\n",
       "## Get started‚Äã\n",
       "\n",
       "Familiarize yourself with LangChain's open-source components by building simple applications.\n",
       "\n",
       "If you're looking to get started with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\n",
       "or other LangChain components from a specific provider, check out our supported [integrations](/docs/integrations/providers/).\n",
       "\n",
       "- [Chat models and prompts](/docs/tutorials/llm_chain/): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates/) and [chat models](/docs/concepts/chat_models/).\n",
       "- [Semantic search](/docs/tutorials/retrievers/): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders/), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).\n",
       "- [Classification](/docs/tutorials/classification/): Classify text into categories or labels using [chat models](/docs/concepts/chat_models/) with [structured outputs](/docs/concepts/structured_outputs/).\n",
       "- [Extraction](/docs/tutorials/extraction/): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models/) and [few-shot examples](/docs/concepts/few_shot_prompting/).\n",
       "\n",
       "Refer to the [how-to guides](/docs/how_to/) for more detail on using all LangChain components.\n",
       "\n",
       "## Orchestration‚Äã\n",
       "\n",
       "Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.\n",
       "\n",
       "- [Chatbots](/docs/tutorials/chatbot/): Build a chatbot that incorporates memory.\n",
       "- [Agents](/docs/tutorials/agents/): Build an agent that interacts with external tools.\n",
       "- [Retrieval Augmented Generation (RAG) Part 1](/docs/tutorials/rag/): Build an application that uses your own documents to inform its responses.\n",
       "- [Retrieval Augmented Generation (RAG) Part 2](/docs/tutorials/qa_chat_history/): Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\n",
       "- [Question-Answering with SQL](/docs/tutorials/sql_qa/): Build a question-answering system that executes SQL queries to inform its responses.\n",
       "- [Summarization](/docs/tutorials/summarization/): Generate summaries of (potentially long) texts.\n",
       "- [Question-Answering with Graph Databases](/docs/tutorials/graph/): Build a question-answering system that queries a graph database to inform its responses.\n",
       "\n",
       "## LangSmith‚Äã\n",
       "\n",
       "LangSmith allows you to closely trace, monitor and evaluate your LLM application.\n",
       "It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n",
       "\n",
       "LangSmith documentation is hosted on a separate site.\n",
       "You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).\n",
       "\n",
       "### Evaluation‚Äã\n",
       "\n",
       "LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\n",
       "\n",
       "- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(docs_with_data_context[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF / DOCX / DOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, vamos a emplear algunos archivos de muestra proporcionados por [Docugami](https://www.docugami.com/). Dichos archivos representan el producto de la extracci√≥n de texto de documentos aut√©nticos, en particular, de archivos PDF relativos a contratos de arrendamiento comercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../data/docugami/commercial_lease/Shorebucks LLC_AZ.xml'),\n",
       " WindowsPath('../data/docugami/commercial_lease/TruTone Lane 1.xml'),\n",
       " WindowsPath('../data/docugami/commercial_lease/TruTone Lane 2.xml')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lease_data_dir = pathlib.Path(\"../data/docugami/commercial_lease\")\n",
    "lease_files = list(lease_data_dir.glob(\"*.xml\"))\n",
    "lease_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, carguemos los documentos de muestra y veamos qu√© propiedades tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded 451 documents.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DocugamiLoader(\n",
    "    docset_id=None,\n",
    "    access_token=None,\n",
    "    document_ids=None,\n",
    "    file_paths=lease_files,\n",
    ")\n",
    "\n",
    "lease_docs = loader.load()\n",
    "f\"Loaded {len(lease_docs)} documents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La metadata obtenida del documento incluye los siguientes elementos:\n",
    "\n",
    "- `id`, `source_id` y `name`: Estos campos identifican de manera un√≠voca al documento y al fragmento de texto que se ha extra√≠do de √©l.\n",
    "- `xpath`: Es el `XPath` correspondiente dentro de la representaci√≥n XML del documento. Se refiere espec√≠ficamente al fragmento extra√≠do. Este campo es √∫til para referenciar directamente las citas del fragmento real dentro del documento XML.\n",
    "- `structure`: Incluye los atributos estructurales del fragmento, tales como `p`, `h1`, `div`, `table`, `td`, entre otros. Es √∫til para filtrar ciertos tipos de fragmentos, en caso de que el usuario los requiera.\n",
    "- `tag`: Representa la etiqueta sem√°ntica para el fragmento. Se genera utilizando diversas t√©cnicas, tanto generativas como extractivas, para determinar el significado del fragmento en cuesti√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xpath': '/dg:chunk/docset:OFFICELEASE-section/dg:chunk',\n",
       " 'id': '14f4ca4426965ae1cef4a5f7b513efa9',\n",
       " 'name': 'Shorebucks LLC_AZ.xml',\n",
       " 'source': 'Shorebucks LLC_AZ.xml',\n",
       " 'structure': 'h1 div',\n",
       " 'tag': 'chunk OFFICELEASE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lease_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docugami` tambi√©n posee la capacidad de asistir en la extracci√≥n de metadatos espec√≠ficos para cada `chunk` o fragmento de nuestros documentos. A continuaci√≥n, se presenta un ejemplo de c√≥mo se extraen y representan estos metadatos:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'xpath': '/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/docset:LeaseParties',\n",
    "    'id': 'v1bvgaozfkak',\n",
    "    'source': 'TruTone Lane 2.docx',\n",
    "    'structure': 'p',\n",
    "    'tag': 'LeaseParties',\n",
    "    'Lease Date': 'April 24 \\n\\n ,',\n",
    "    'Landlord': 'BUBBA CENTER PARTNERSHIP',\n",
    "    'Tenant': 'Truetone Lane LLC',\n",
    "    'Lease Parties': 'Este ACUERDO DE ARRENDAMIENTO DE OFICINA (el \"Contrato\") es celebrado por y entre BUBBA CENTER PARTNERSHIP (\"Arrendador\"), y Truetone Lane LLC, una compa√±√≠a de responsabilidad limitada de Delaware (\"Arrendatario\").'\n",
    "}\n",
    "```\n",
    "\n",
    "Los metadatos adicionales, como los mostrados arriba, pueden ser extremadamente √∫tiles cuando se implementan `self-retrievers`, los cuales ser√°n explorados adetalle m√°s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga tus documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prefieres utilizar tus propios documentos, puedes cargarlos a trav√©s de la interfaz gr√°fica de [Docugami](https://www.docugami.com/). Una vez cargados, necesitar√°s asignar cada uno a un `docset`. Un `docset` es un conjunto de documentos que presentan una estructura an√°loga. Por ejemplo, todos los contratos de arrendamiento comercial por lo general poseen estructuras similares, por lo que pueden ser agrupados en un √∫nico `docset`.\n",
    "\n",
    "Despu√©s de crear tu `docset`, los documentos cargados ser√°n procesados y estar√°n disponibles para su acceso mediante la API de `Docugami`.\n",
    "\n",
    "Para recuperar los `ids` de tus documentos y de sus correspondientes `docsets`, puedes ejecutar el siguiente comando:\n",
    "\n",
    "```bash\n",
    "curl --header \"Authorization: Bearer {YOUR_DOCUGAMI_TOKEN}\" \\\n",
    "  https://api.docugami.com/v1preview1/documents\n",
    "```\n",
    "\n",
    "Este comando te facilitar√° el acceso a la informaci√≥n relevante, optimizando as√≠ la administraci√≥n y organizaci√≥n de tus documentos dentro de `Docugami`.\n",
    "\n",
    "Una vez hayas extra√≠do los `ids` de tus documentos o de los `docsets`, podr√°s emplearlos para acceder a la informaci√≥n de tus documentos utilizando el `DocugamiLoader` de `Langchain`. Esto te permitir√° manipular y gestionar tus documentos dentro de tu aplicaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "DOCUGAMI_API_KEY = os.environ.get(\"DOCUGAMI_API_KEY\")\n",
    "loader = DocugamiLoader(\n",
    "    docset_id=\"p6ycisj88ple\",\n",
    "    access_token=DOCUGAMI_API_KEY,\n",
    "    document_ids=None,\n",
    "    file_paths=None,\n",
    ")\n",
    "\n",
    "papers_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk PsychologicalInsight\n",
      "Adulaci√≥nvs.Apreciaci√≥n\n",
      "EmotionalExpression\n",
      "Self-ImprovementAdvice\n",
      "chunk InsufficientInformation\n",
      "chunk\n",
      "chunk HumorAnecdote\n",
      "chunk CharacterBehaviorDescription\n",
      "chunk BullfightersJoy\n",
      "CharacterList\n",
      "chunk InsufficientInformation\n"
     ]
    }
   ],
   "source": [
    "lost_in_the_middle_paper_docs = [\n",
    "    doc for doc in papers_docs if doc.metadata[\"source\"] == \"influence.pdf\"\n",
    "]\n",
    "for doc in lost_in_the_middle_paper_docs:\n",
    "    print(doc.metadata[\"tag\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
