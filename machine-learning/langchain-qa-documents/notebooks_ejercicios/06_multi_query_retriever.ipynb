{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-query retriever\n",
    "\n",
    "![Multi-query retriever](../diagrams/slide_diagrama_03_V2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.langchain_docs_loader import load_langchain_docs_splitted\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_langchain_docs_splitted()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prueba de retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multy_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"How to create a retriever with langchain expression language?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de preguntas alternativas de forma personalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de esquema de salida de preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de `prompt` personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an AI language assistant well versed in the Langchain Documentation.\n",
    "Your more precise task is to generate five different versions of the given question to retrieve relevant documents from a vector database.\n",
    "By generating multiple perspectives on the question, your goal is to overcome some of the limitations of the distance-based similarity search.\n",
    "\n",
    "Provide these alternative questions separed by newlines.\n",
    "\n",
    "Original question: {question}\n",
    "New questions:\"\"\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    prompt=prompt,\n",
    "    output_parser=,\n",
    ")\n",
    "\n",
    "# In language expression language, you could create the chain with:\n",
    "# llm_chain = prompt | llm | LineListOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use de cadena de generación de preguntas personalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke(\n",
    "    {\"question\": \"How to create a retriever with langchain expression language?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integración de cadena de generación de preguntas personalizada en `retriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de `retriever` con cadena de generación de preguntas personalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"How to create a retriever with lagnchain expression language?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
