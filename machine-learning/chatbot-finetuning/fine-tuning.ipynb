{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fecda0f-7374-42fa-be48-5124846ef732",
   "metadata": {},
   "source": [
    "## Tokenización de texto para calcular costos de uso de OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c9036-186b-4712-8a67-134d714d0e5c",
   "metadata": {},
   "source": [
    "### Instalación de titktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5997f33b-3049-4497-ab90-8025032b2b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\anaconda\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\anaconda\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\anaconda\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c19ce22-5fd6-4b58-be8a-1391481dbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f603f25-7e67-407e-882e-e3ddbc75dae8",
   "metadata": {},
   "source": [
    "### Selección y carga de encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bab4c-80fb-4e9a-afdd-45d957f819fe",
   "metadata": {},
   "source": [
    "cl100k_base para modelos gpt-4, gpt-3.5-turbo, text-embedding-ada-002.\n",
    "\n",
    "Consulta el <a>https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</a>  para otro tipo de modelos o cambios en este encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c4d532-128c-4435-b849-391114f85905",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7185f1b-6e94-49eb-86b4-a0b444bdf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a2210-2dda-4ce9-8ce9-ca823830130f",
   "metadata": {},
   "source": [
    "### Tokenización de string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902263a0-e746-42a8-9910-5160a44cce9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6719, 281, 618, 6491, 2586, 2092, 6491, 13]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"El perrito come solito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbf4f61-9b9f-4074-9698-d5397b10129c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(\"El perrito come solito.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc05bf6-5611-4c0b-8845-d51d36ab10f7",
   "metadata": {},
   "source": [
    "### Tokenizar una serie de mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a97ebdd-efc0-4388-b69f-805a0550b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b41da2-3031-4d4b-b26d-c4a542935bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"Eres un asistente de atención a clientes y estudiantes de la plataforma de educación online en tecnología, inglés y liderazgo llamada Platzi\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿Cómo puedo mejorar mis habilidades de creación de contenido con inteligencia artificial?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Para mejorar tus habilidades de creación de contenido con inteligencia artificial te sugiero que tomes el Taller de Creación de Contenido con Inteligencia Artificial en https://platzi.com/cursos/contenido-ia/. En este curso aprenderás a utilizar herramientas para generar contenido de manera automatizada y eficiente. ¡No te lo pierdas!\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e33205a1-f88f-46af-a88b-db3aec0120b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_messages(example_messages, model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946120fa-64fa-46b5-ad2a-a652d8f5cbee",
   "metadata": {},
   "source": [
    "## Preparación de datos y análisis para fine-tuning de chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e2006b-7369-4ffc-8871-584e3c672e53",
   "metadata": {},
   "source": [
    "<a>https://cookbook.openai.com/examples/chat_finetuning_data_prep</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f06ead-f1ef-4317-9f52-f78acfcf7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # para conteo de Tokens\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950776e0-60fe-40a0-a6c3-83b23790a14e",
   "metadata": {},
   "source": [
    "### Cargar dataset desde archivo JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2f1795-f976-4c94-8de9-5cd9f0dab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data_train.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7704a28c-c2b5-4cf4-bc4f-40fd38cb30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 1261\n",
      "First example:\n",
      "{'role': 'system', 'content': 'Eres un asistente de atención a clientes y estudiantes de la plataforma de educación online en tecnología, inglés y liderazgo llamada Platzi'}\n",
      "{'role': 'user', 'content': '¿Cómo puedo mejorar mis habilidades en JavaScript?,'}\n",
      "{'role': 'assistant', 'content': 'Para mejorar tus habilidades en JavaScript, te recomendamos tomar el Curso Práctico de JavaScript en https://platzi.com/cursos/javascript-practico/. También puedes practicar en proyectos personales y buscar recursos en línea para continuar aprendiendo. ¡Mucho éxito en tu camino de aprendizaje de JavaScript!'}\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Análisis inicial del dataset\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24873ed7-c78f-4e1b-8c82-f88a3425bc7f",
   "metadata": {},
   "source": [
    "### Validacion del Formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40f1bdb2-4912-4d76-a7f4-593abd4faec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Revisión de errores de formato\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e053d3-8748-4d1b-8f47-c632d2ea21bd",
   "metadata": {},
   "source": [
    "### Utilidades de conteo de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a8cc8e-8c74-4c13-ae47-5b46fafe55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50be8d-2ddc-4f89-9030-bed4fb69e249",
   "metadata": {},
   "source": [
    "### Conteo de Tokens y advertencias de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "963f75e1-8e4b-408e-93d4-559c28c29ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 68, 206\n",
      "mean / median: 133.87946074544013, 133.0\n",
      "p5 / p95: 114.0, 159.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 9, 132\n",
      "mean / median: 69.51704996034893, 69.0\n",
      "p5 / p95: 50.0, 92.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Advertencias y conteo de tokens\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2404b14-942d-4127-affc-0c89e0ce1415",
   "metadata": {},
   "source": [
    "### Estimación de costos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c0773dc-0f2a-4006-aa0d-9b02c7f4b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~168822 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~506466 tokens\n"
     ]
    }
   ],
   "source": [
    "# Costo y estimado de número de épocas por defecto\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd93ac-b3b2-41ed-983f-8ad7ce6f5bdc",
   "metadata": {},
   "source": [
    "## Importar librería de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5be60bd8-4cd4-4786-ab3c-bf072f3ebf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='INGRESA TU API KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf958c-c6d5-4b50-9653-3ff3d6b20a50",
   "metadata": {},
   "source": [
    "### Cargar datasets\n",
    "Carga los archivos de los datasets a la plataforma de OpenAI. Esto te generará un ID del archivo que usarás para el fine-tuning del modelo base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1987d2b-cf37-47c9-aa6c-880aeb293941",
   "metadata": {},
   "source": [
    "### Dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f9508-f32d-4f88-a355-f46e47d53972",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "  file=open(\"INGRESA EL PATH DE data_train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f3db1-9646-481e-b9c8-55cee36a7547",
   "metadata": {},
   "source": [
    "### Dataset de validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4c798-3c2f-4c00-8dd6-e598a2d501aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "  file=open(\"INGRESA EL PATH DE data_val.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91faaee8-6106-4438-8578-1ae7890a4178",
   "metadata": {},
   "source": [
    "### Fine-tuning de modelo base\n",
    "Crea un proceso de fine-tuning.\n",
    "\n",
    "Necesitas el id de tus dos archivos de datasets de entrenamiento y validación.\n",
    "Además del nombre del modelo base.\n",
    "Este código iniciará el proceso de fine-tuning que podrás darle seguimiento en la plataforma de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf572c-1328-4eed-aa7b-1ad82a811cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"INGRESA EL FILE ID DEL ARCHIVO DE TRAIN\", \n",
    "  validation_file='INGRESA EL FILE ID DEL ARCHIVO DE VAL',\n",
    "  model=\"gpt-3.5-turbo-1106\" #Puedes cambiar el modelo base según lo necesites.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a689b-c851-48f1-8319-3e7956a6dfe2",
   "metadata": {},
   "source": [
    "### Listar y eliminar modelos con fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66700ec-d090-4db2-84e8-40c02dac6d1f",
   "metadata": {},
   "source": [
    "### Listar modelos en tu organizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33795ecb-4df5-447f-a6db-c4d8403e53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.list(limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978aa75-2d11-40da-aff9-69f0e48b9d77",
   "metadata": {},
   "source": [
    "### Eliminar modelos de tu organizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1744e-df14-48ef-b6ac-d6d773069855",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.delete(\"INGRESA EL NOMBRE DEL MODELO A ELIMINAR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf845a-37c1-4fba-861c-a8fb0b375cf3",
   "metadata": {},
   "source": [
    "## Análisis del modelo con fine-tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ab372-0281-43fe-9fed-54b595c51919",
   "metadata": {},
   "source": [
    "Fuente: <a>https://platform.openai.com/docs/guides/fine-tuning/analyzing-your-fine-tuned-model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "673bd43b-9a24-483d-8c2c-be32efd595ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\anaconda\\lib\\site-packages (1.61.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\anaconda\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\anaconda\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\anaconda\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\anaconda\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\anaconda\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\anaconda\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\anaconda\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\anaconda\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\anaconda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621fd52-fe32-48cb-bfc1-d7b75fdc2304",
   "metadata": {},
   "source": [
    "### Uso de modelo con fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f57a72-395e-47c0-9f45-e8bcaab353cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Código exportado del Playground de OpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='INSERTA TU API KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df9ffc-7041-44cd-ad5c-4573748ac856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f5910-2e04-4771-a2ca-a26479d19106",
   "metadata": {},
   "source": [
    "### Carga de modelo con fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a4af2-d7cc-4fe4-aec5-395b486a7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='INSERTA TU API KEY')\n",
    "\n",
    "client.fine_tuning.jobs.retrieve(\"INSERTA EL JOB ID DE TU MODELO CON FINE-TUNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80973cd1-a93d-44a8-af2a-44c3315aed64",
   "metadata": {},
   "source": [
    "### Obtener archivo de resultados de fine-tuning de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d6238-0273-48e0-a9ab-90d3e39afb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = client.files.content('INSERTA EL NOMBRE DE TU ARCHIVO DE METRICAS')\n",
    "\n",
    "content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681659a5-cb63-4744-a725-745d817be294",
   "metadata": {},
   "source": [
    "### Interpretación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ead1e4-482e-453d-94d5-98b603cacb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fc9e3-da5d-4720-b99c-dca5f142d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_str = content.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cea0b-7c5c-4537-aca8-5db3ed11433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = [line.split(',') for line in metrics_str.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f783fd-fe0a-40ec-8156-46c754b83dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metrics_list[1:], columns=metrics_list[0])\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521fa80-1994-4fe6-8a4f-0b3d81fe6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversión a numéricos para graficación\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb228497-9118-430e-87c0-2f599f5a4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df['step'], df['train_accuracy'])\n",
    "plt.title('Training Accuracy over Steps')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5694d-641a-42a0-b07b-2a135469f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df['step'], df['train_loss'])\n",
    "plt.title('Training Loss over Steps')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
